# My-Data-Journey ğŸš€

Bienvenue sur mon portfolio technique ! ğŸ‘‹

DiplÃ´mÃ© en Informatique (BUT & Bachelor UQAC), je consacre actuellement une annÃ©e de cÃ©sure Ã  ma spÃ©cialisation intensive en **Data Engineering** et **Cloud Computing**.

Ce dÃ©pÃ´t centralise mes projets d'apprentissage, mes POCs (Proof of Concepts) et mes pipelines de donnÃ©es, documentant ma progression vers les certifications **Azure** et **Databricks**.

## ğŸ¯ Objectifs & Stack Technique

Mon focus se porte sur la "Modern Data Stack" et l'Ã©cosystÃ¨me Cloud :

| Domaine | Technologies & Outils |
| :--- | :--- |
| **Langages** | ğŸ Python, ğŸ—ƒï¸ SQL |
| **Processing** | ğŸ¼ Pandas, ğŸ¹ PyArrow, âš¡ Spark (Ã€ venir) |
| **Formats** | ğŸ“„ CSV (Bronze), ğŸ“¦ Parquet (Silver/Gold) |
| **Cloud** | â˜ï¸ Microsoft Azure (Data Lake Gen2, Storage Account) |
| **Architecture** | ğŸ… Medallion Architecture (Bronze/Silver/Gold) |
| **QualitÃ© & CI/CD** | ğŸ” Dotenv (SÃ©curitÃ©), ğŸ—ï¸ Git |

## ğŸ“‚ Projets RÃ©alisÃ©s

### 1. Crypto Data Pipeline (Architecture Medallion)
*Pipeline ETL complet : De l'ingestion brute Ã  l'agrÃ©gation de KPIs.*

Ce projet implÃ©mente une **Architecture Medallion** (standard Databricks) pour traiter des donnÃ©es financiÃ¨res simulÃ©es. Il dÃ©montre la capacitÃ© Ã  transformer des donnÃ©es brutes en insights mÃ©tier via un pipeline automatisÃ©.

- **Architecture :**
  - **ğŸ¥‰ Couche Bronze (Raw) :** Ingestion de donnÃ©es brutes au format CSV.
  - **ğŸ¥ˆ Couche Silver (Cleansed) :** Nettoyage, typage strict et conversion en format **Parquet** (optimisation du stockage et performance de lecture).
  - **ğŸ¥‡ Couche Gold (Aggregated) :** Calcul de KPIs (Moyennes, Totaux) pour usage Business/BI.
  - **ğŸ¤– Orchestration :** Script Python maitre pilotant l'exÃ©cution sÃ©quentielle des tÃ¢ches ETL.

- **CompÃ©tences clÃ©s :**
    - **Data Transformation :** Manipulation avancÃ©e avec Pandas (Nettoyage, Cast, GroupBy).
    - **Storage Optimization :** Passage du CSV (Row-based) au Parquet (Columnar) pour simuler les bonnes pratiques Big Data.
    - **SÃ©curitÃ© :** Gestion des clÃ©s d'accÃ¨s via variables d'environnement (`.env`).
    
- **Stack :** Python, Pandas, PyArrow, Azure Storage Blob.
- **Statut :** âœ… V1 (Local Pipeline) TerminÃ©e
- **Lien :** [Voir le code source](./crypto_ingestion)
- **Documentation :** [ğŸ“˜ Lire la Documentation Technique (PDF)](./crypto_ingestion/docs/Documentation_Pipeline_d_Ingestion_Crypto_vers_Azure_Data_Lake.pdf)

*(Prochaines Ã©tapes : Migration vers Azure Data Factory & Visualisation Power BI)*

## ğŸ† Certifications VisÃ©es

- [ ] **Microsoft Azure Data Fundamentals (DP-900)** *(En cours de prÃ©paration)*
- [ ] **Databricks Lakehouse Fundamentals**
- [ ] **Databricks Data Engineer Associate**
- [ ] **Azure Data Engineer Associate (DP-203)**

---
*Ce portfolio est maintenu par Yoann LEHONG CHEFFSON. N'hÃ©sitez pas Ã  explorer le code pour voir ma logique d'ingÃ©nierie !*
